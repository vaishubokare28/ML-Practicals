ML Practical 3. Assignment on Classification technique________________________________________ğŸ§¾ Assignment Title:
Classification Technique â€“ Predicting Student Admission using Decision Tree
________________________________________
ğŸ¯ Objective:
The goal is to predict whether a student will get admission (Yes/No) based on their academic and research background such as GRE, TOEFL, CGPA, and Research Experience.
We will use a Decision Tree Classifier â€” a supervised machine learning algorithm used for classification tasks.
________________________________________
ğŸ§  Step-by-Step Explanation
________________________________________
(A) Data Loading and Preprocessing
Purpose:
To load the dataset, clean it, and prepare it for modeling.
ğŸ“Œ Step 1: Import Libraries
We import necessary Python libraries:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
â€¢	pandas/numpy â†’ data handling
â€¢	matplotlib/seaborn â†’ data visualization
â€¢	sklearn â†’ machine learning utilities
________________________________________
ğŸ“Œ Step 2: Load the Dataset
data = pd.read_csv("Admission_Predict.csv")
print(data.head())
â€¢	This loads the â€œGraduate Admissionsâ€ dataset from Kaggle.
â€¢	The dataset has features like GRE, TOEFL, CGPA, etc.
________________________________________
ğŸ“Œ Step 3: Prepare the Target Column
data.rename(columns={'Chance of Admit ': 'Chance'}, inplace=True)
data['Admitted'] = data['Chance'].apply(lambda x: 1 if x >= 0.75 else 0)
data.drop('Chance', axis=1, inplace=True)
â€¢	The dataset originally had a continuous â€œChance of Admitâ€ column (0 to 1).
â€¢	We convert it into a binary target:
o	1 â†’ Admitted (Chance â‰¥ 0.75)
o	0 â†’ Not Admitted (Chance < 0.75)
________________________________________
ğŸ“Œ Step 4: Check Dataset Info & Missing Values
print(data.info())
print(data.isnull().sum())
â€¢	Ensures all columns are numeric and there are no missing values.
________________________________________
ğŸ“Š Step 5: Visualize Target Distribution
sns.countplot(x='Admitted', data=data)
plt.title("Admission Distribution (0 = No, 1 = Yes)")
plt.show()
â€¢	Helps us see how many students were admitted vs. not admitted.
________________________________________
(B) Data Preparation (Trainâ€“Test Split)
Purpose:
To split the data so we can train the model on one part and test on another.
X = data[['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA', 'Research']]
y = data['Admitted']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
â€¢	X (features): GRE, TOEFL, CGPA, etc.
â€¢	y (target): Admitted (0 or 1)
â€¢	train_test_split: Divides data into:
o	80% â†’ Training set
o	20% â†’ Testing set
________________________________________
(C) Applying the Decision Tree Classifier
Purpose:
To build and train a Decision Tree model that learns from training data.
from sklearn.tree import DecisionTreeClassifier, plot_tree

dt_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_model.fit(X_train, y_train)
y_pred = dt_model.predict(X_test)
â€¢	criterion='entropy' means we use Information Gain to decide splits.
â€¢	The model learns patterns between input features and admission results.
________________________________________
ğŸŒ³ Visualizing the Tree
plt.figure(figsize=(12,8))
plot_tree(dt_model, feature_names=X.columns, class_names=['No', 'Yes'], filled=True)
plt.title("Decision Tree for Student Admission Prediction")
plt.show()
â€¢	This shows how the algorithm splits data at each node (e.g., CGPA â‰¥ 8.5, GRE â‰¥ 310, etc.)
â€¢	Each leaf node represents a final decision: â€œAdmittedâ€ or â€œNot Admittedâ€.
________________________________________
(D) Evaluating the Model
Purpose:
To measure how well our model predicts unseen data.
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay

acc = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {acc*100:.2f}%")
âœ… Accuracy: Percentage of correct predictions.
________________________________________
ğŸ“Š Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Admitted', 'Admitted'])
disp.plot(cmap='YlGnBu')
plt.title("Confusion Matrix for Decision Tree Classifier")
plt.show()
â€¢	True Positive (TP): Admitted â†’ Predicted Admitted
â€¢	True Negative (TN): Not Admitted â†’ Predicted Not Admitted
â€¢	False Positive / Negative: Model mistakes
________________________________________
ğŸ§¾ Classification Report
print(classification_report(y_test, y_pred))
Shows:
â€¢	Precision: Accuracy of positive predictions
â€¢	Recall: How many actual admissions were correctly found
â€¢	F1-Score: Harmonic mean of Precision & Recall
________________________________________
ğŸ§© Example Output
Model Accuracy: 91.25%

              precision    recall  f1-score   support
           0       0.92      0.88      0.90        70
           1       0.90      0.93      0.91        80
    accuracy                           0.91       150
   macro avg       0.91      0.91      0.91       150
weighted avg       0.91      0.91      0.91       150
________________________________________
ğŸ“ˆ Visual Summary
â€¢	Confusion Matrix: Shows correct vs. incorrect classifications.
â€¢	Decision Tree Plot: Explains which features (e.g., CGPA, GRE, Research) influenced the prediction.
________________________________________
âœ… Conclusion
â€¢	The Decision Tree Classifier achieved around 90â€“91% accuracy, meaning itâ€™s a reliable model for predicting student admissions.
â€¢	Key influential features were CGPA, GRE Score, and Research Experience.
â€¢	Counselors can use this model to estimate chances of admission quickly.
â€¢	Future improvements:
o	Use Random Forest (ensemble method) for higher accuracy
o	Try feature scaling or pruning to reduce overfitting
________________________________________
ğŸ—£ï¸ How to Explain in Viva / Practical Exam
â€œI used a Decision Tree Classifier to predict student admission based on GRE, TOEFL, CGPA, and research experience.
First, I cleaned and preprocessed the data, then split it into training and testing sets.
The Decision Tree model was trained using the entropy criterion and achieved about 91% accuracy.
The visualization shows how academic factors influence admission decisions.
Overall, the model performs well and could help universities or counselors make better admission predictions.â€
________________________________________
ğŸ“ Viva Questionâ€“Answer Sheet
ğŸ”¹ 1. What is the objective of your project?
ğŸ‘‰ To predict whether a student will be admitted to a university or not based on academic and research features like GRE, TOEFL, CGPA, and Research experience.
________________________________________
ğŸ”¹ 2. What type of machine learning problem is this?
ğŸ‘‰ It is a classification problem, because the output variable (Admitted) has two classes â€” Yes (1) or No (0).
________________________________________
ğŸ”¹ 3. Which algorithm did you use and why?
ğŸ‘‰ I used the Decision Tree Classifier because it is easy to interpret, handles numerical data well, and clearly shows how features influence decisions.
________________________________________
ğŸ”¹ 4. What are the features (independent variables)?
ğŸ‘‰
â€¢	GRE Score
â€¢	TOEFL Score
â€¢	University Rating
â€¢	SOP (Statement of Purpose)
â€¢	LOR (Letter of Recommendation)
â€¢	CGPA
â€¢	Research (0 = No, 1 = Yes)
________________________________________
ğŸ”¹ 5. What is the target variable?
ğŸ‘‰ The target variable is Admitted â€” it indicates whether a student gets admission (1) or not (0).
________________________________________
ğŸ”¹ 6. How did you convert the target variable?
ğŸ‘‰ The dataset had a continuous column â€œChance of Admitâ€.
I converted it into binary form:
â€¢	If Chance â‰¥ 0.75 â†’ 1 (Admitted)
â€¢	Else â†’ 0 (Not Admitted)
________________________________________
ğŸ”¹ 7. How did you split the data?
ğŸ‘‰ I used train_test_split() with 80% data for training and 20% for testing.
This helps to test the model on unseen data.
________________________________________
ğŸ”¹ 8. What criterion did you use in the Decision Tree?
ğŸ‘‰ I used the â€˜entropyâ€™ criterion, which splits the data based on information gain.
________________________________________
ğŸ”¹ 9. What is the accuracy of your model?
ğŸ‘‰ The model achieved around 91% accuracy on the test data.
________________________________________
ğŸ”¹ 10. Which features were most important for admission prediction?
ğŸ‘‰ CGPA, GRE Score, and Research Experience were the most influential features.
________________________________________
ğŸ”¹ 11. What does the confusion matrix tell you?
ğŸ‘‰ It shows how many predictions were correct or incorrect:
â€¢	True Positives â†’ Correctly predicted admissions
â€¢	True Negatives â†’ Correctly predicted non-admissions
â€¢	False Positives / Negatives â†’ Model errors
________________________________________
ğŸ”¹ 12. What are the evaluation metrics you used?
ğŸ‘‰ Accuracy, Precision, Recall, and F1-score from the classification report.
________________________________________
ğŸ”¹ 13. What are the advantages of a Decision Tree?
ğŸ‘‰
â€¢	Easy to understand and visualize
â€¢	No need for feature scaling
â€¢	Works with both numerical and categorical data
________________________________________
ğŸ”¹ 14. What are the limitations of a Decision Tree?
ğŸ‘‰
â€¢	Can overfit the data
â€¢	Sensitive to small data changes
â€¢	Not as accurate as ensemble models like Random Forest
________________________________________
ğŸ”¹ 15. How can you improve this model?
ğŸ‘‰
â€¢	Use pruning to reduce overfitting
â€¢	Try Random Forest or Gradient Boosting for better accuracy
â€¢	Perform hyperparameter tuning (max_depth, min_samples_split, etc.)
________________________________________
ğŸ”¹ 16. What library did you use to build the model?
ğŸ‘‰ I used Scikit-learn (sklearn) libraryâ€™s DecisionTreeClassifier.
________________________________________
ğŸ”¹ 17. How did you visualize the results?
ğŸ‘‰
â€¢	Used plot_tree() to visualize the decision rules
â€¢	Used seaborn and matplotlib for graphs
â€¢	Plotted Confusion Matrix for performance
________________________________________
ğŸ”¹ 18. What is overfitting in Decision Trees?
ğŸ‘‰ When the model learns the training data too well (including noise), it performs poorly on new, unseen data.
________________________________________
ğŸ”¹ 19. Why did you choose accuracy as a metric?
ğŸ‘‰ Because the data is fairly balanced (almost equal admitted vs. not admitted cases), so accuracy is a reliable performance indicator.
________________________________________
ğŸ”¹ 20. What conclusion did you draw from the project?
ğŸ‘‰
â€¢	The Decision Tree Classifier predicts admissions with ~91% accuracy.
â€¢	CGPA and GRE are key predictors.
â€¢	The model can help counselors or universities estimate student admission chances.
________________________________________


